{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Games Sales Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost==1.7.0\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "m8YHb_hYlkFj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from xgboost.spark import SparkXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc=SparkContext(master=\"local[*]\")\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"Python Spark SQL basic example\") \\\n",
    "#         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load('data/Video_Games_Sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (16719, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"df shape:\", (df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year_of_Release: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: string (nullable = true)\n",
      " |-- EU_Sales: string (nullable = true)\n",
      " |-- JP_Sales: string (nullable = true)\n",
      " |-- Other_Sales: string (nullable = true)\n",
      " |-- Global_Sales: string (nullable = true)\n",
      " |-- Critic_Score: string (nullable = true)\n",
      " |-- Critic_Count: string (nullable = true)\n",
      " |-- User_Score: string (nullable = true)\n",
      " |-- User_Count: string (nullable = true)\n",
      " |-- Developer: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some N/A's appear as strings, so we'll replace them with NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([when(col(c) == \"N/A\", \"\").otherwise(col(c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "|Name|Platform|Year_of_Release|Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|Critic_Score|Critic_Count|User_Score|User_Count|Developer|Rating|\n",
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "|   2|       0|            269|    2|       54|       0|       0|       0|          0|           0|        8582|        8582|      9129|      9129|     6623|  6769|\n",
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_counts = df.select([sum(when(isnull(c) | isnan(c) | (col(c) == \"\"), 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Name__: this column has only 2 missing valus, we can easily drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Name'].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Year of relese__: is one of the core components for this reaserch. It cant be replaced with an average or even an everage span of years from the console activity years because most of the used platforms used to release games for more than 7 years. Plus, more than 40% are also missing critics scores. Missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Platform|\n",
      "+--------+\n",
      "|      PC|\n",
      "|     PS3|\n",
      "|      PS|\n",
      "|     PS2|\n",
      "|     3DS|\n",
      "|      GB|\n",
      "|     N64|\n",
      "|     PSP|\n",
      "|    2600|\n",
      "|    X360|\n",
      "|     GBA|\n",
      "|     Wii|\n",
      "|      GC|\n",
      "|     PSV|\n",
      "|      XB|\n",
      "|      DS|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "platforms = df.filter(df['Year_of_Release'].isNull()).groupBy('Platform').agg({'Platform': 'count'}).select('Platform').distinct()\n",
    "platforms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Year_of_Release'].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also 2020 has only one record with lots of missing data, it will be removed. Same for 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Year_of_Release'] != 2020)\n",
    "df = df.filter(df['Year_of_Release'] != 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ratings__: The American game rating system is called ESRB (Entertainment Software Rating Board). \n",
    "It provides information about what's in a game or app so parents and consumers can make informed choices about which games are right for their family. More information can be found on the official website HERE.\n",
    "\n",
    "Regarding the replacement of empty or missing values:\n",
    "* Since the rating system was created on 1994, we will replace empty values only after 1994. \n",
    "* NA values from before 1994 will be filled with rating E since it was for evetyone. \n",
    "* NA Values after 1994 will be left as N/A.\n",
    "* Scare values like Ao (Adults Only), RP (Rating Pending), K-A (Kids to Adults), EC (Early Childhood), and N/A (No Rating) will be replaced according to this mapping:\n",
    "\n",
    "| Rating | Map | Description |\n",
    "| ------ | ------ | ------ |\n",
    "| EC | E | Early childhood |\n",
    "| K-A | E | Kids to Adults, so basically everyone|\n",
    "| AO | M | Adults Only |\n",
    "| RP | E10+ | We checked the game rating online |\n",
    "| N/A | NR | No Rating |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace scare values\n",
    "df = df.withColumn('Rating', when(df['Rating'].isin('EC', 'K-A'), 'E')\n",
    "                            .when(df['Rating'] == 'AO', 'M')\n",
    "                            .when(df['Rating'].isin('RP'), 'E10+')\n",
    "                            .when(df['Rating'].isNull(), 'NR')\n",
    "                            .otherwise(df['Rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Rating', when(df['Rating'].isNull(), 'E').otherwise(df['Rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Developer__: Since this column has sub-important information for our reaserch, has ~40% missing data and 1723 unique string values it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 11:14:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Number of unique values in the 'Developer' column:  1723\n"
     ]
    }
   ],
   "source": [
    "nunique = df.agg(approx_count_distinct(df['Developer'])).collect()[0][0]\n",
    "print(\"Number of unique values in the 'Developer' column: \", nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of missing data in the 'Developer' column:  39.76%\n"
     ]
    }
   ],
   "source": [
    "result = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"Developer\"]]).collect()[0][0] / df.count()\n",
    "print(\"Precentage of missing data in the 'Developer' column: \", '{:.2%}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Developer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Publisher__: Since it has only 2 missing values it wont be dropped. Also this column may prove importent as a publisher has big role in game sales. we'll replace the missing values with __'Small_Publisher'__. Small publisher defined as companies that revenued less than 0.5M$ over a year in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Publisher\", when(col(\"Publisher\").isNull(), \"Small_Publisher\").otherwise(col(\"Publisher\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__User_Count & Critic_Count__: Since this values report no critic, thus will be filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"User_Count\", when(df[\"User_Count\"].isNull(), 0).otherwise(df[\"User_Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Critic_Count\", when(df[\"Critic_Count\"].isNull(), 0).otherwise(df[\"Critic_Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Critic_Score and User_Score__: This columns may be crucial in our reasecrh, thus filling this NA's is important and should be handled with care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_scores = df.filter(col(\"Critic_Score\").isNull()).groupBy(\"Year_of_Release\").count()\n",
    "missing_scores = missing_scores.toPandas()\n",
    "missing_scores = missing_scores.set_index('Year_of_Release')\n",
    "missing_scores = missing_scores.sort_index()\n",
    "\n",
    "missing_scores.plot(kind='bar')\n",
    "plt.title('Missing Critic Scores Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Missing Critic Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+----------+-----+----+\n",
      "|Critic_Score|Critics|User_Score|Users|Year|\n",
      "+------------+-------+----------+-----+----+\n",
      "|         0.0|      0|       0.0|    0|1980|\n",
      "|         0.0|      0|       0.0|    0|1981|\n",
      "|         0.0|      0|       0.0|    0|1982|\n",
      "|         0.0|      0|       0.0|    0|1983|\n",
      "|         0.0|      0|       0.0|    0|1984|\n",
      "|        59.0|      1|       6.0|    1|1985|\n",
      "|         0.0|      0|       0.0|    0|1986|\n",
      "|         0.0|      0|       0.0|    0|1987|\n",
      "|        64.0|      1|       2.0|    1|1988|\n",
      "|         0.0|      0|       0.0|    0|1989|\n",
      "|         0.0|      0|       0.0|    0|1990|\n",
      "|         0.0|      0|       0.0|    0|1991|\n",
      "|        85.0|      1|       8.0|    1|1992|\n",
      "|         0.0|      0|       0.0|    0|1993|\n",
      "|        69.0|      1|       6.0|    1|1994|\n",
      "|         0.0|      0|       0.0|    0|1995|\n",
      "|        90.0|      8|       8.0|    8|1996|\n",
      "|        85.0|     17|       8.0|   18|1997|\n",
      "|        82.0|     28|       9.0|   31|1998|\n",
      "|        76.0|     39|       8.0|   31|1999|\n",
      "|        69.0|    143|       8.0|  107|2000|\n",
      "|        71.0|    326|       8.0|  257|2001|\n",
      "|        69.0|    627|       8.0|  464|2002|\n",
      "|        70.0|    585|       8.0|  516|2003|\n",
      "|        69.0|    561|       8.0|  489|2004|\n",
      "|        69.0|    655|       8.0|  583|2005|\n",
      "|        67.0|    620|       7.0|  552|2006|\n",
      "|        66.0|    692|       7.0|  629|2007|\n",
      "|        66.0|    715|       7.0|  661|2008|\n",
      "|        68.0|    651|       7.0|  614|2009|\n",
      "|        67.0|    500|       7.0|  467|2010|\n",
      "|        69.0|    500|       7.0|  501|2011|\n",
      "|        73.0|    321|       7.0|  339|2012|\n",
      "|        71.0|    273|       6.0|  304|2013|\n",
      "|        71.0|    261|       7.0|  329|2014|\n",
      "|        73.0|    225|       7.0|  297|2015|\n",
      "|        73.0|    232|       7.0|  262|2016|\n",
      "+------------+-------+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "years = df.select(\"Year_of_Release\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "years.sort()\n",
    "\n",
    "average_score_map = []\n",
    "\n",
    "for year in years:\n",
    "    year = int(year)\n",
    "    year_df = df.filter(col(\"Year_of_Release\") == year)\n",
    "\n",
    "    mean_Critic_Score = year_df.select(round(mean(col(\"Critic_Score\")))).collect()[0][0] or 0.0\n",
    "    mean_User_Score = year_df.select(round(mean(col(\"User_Score\")))).collect()[0][0] or 0.0\n",
    "\n",
    "    average_score_map.append({\n",
    "        \"Year\": year,\n",
    "        \"Critic_Score\": mean_Critic_Score,\n",
    "        \"Critics\": year_df.filter(col(\"Critic_Score\").isNotNull()).count(),\n",
    "        \"User_Score\": mean_User_Score,\n",
    "        \"Users\": year_df.filter(col(\"User_Score\").isNotNull()).count()\n",
    "    })\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(average_score_map)\n",
    "\n",
    "# Convert the RDD to a DataFrame\n",
    "scores_df = rdd.toDF([\"Critic_Score\", \"Critics\", \"User_Score\", \"Users\", \"Year\"])\n",
    "scores_df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 2000 scores have no meaning, because they are submitted by a too low percentage of the records and shouldn't have effect on the machines learning procces.\n",
    "From 2000 onward we will fill the NaN values with Yearly averge. in that way we fill the NaN and still bearly effect the yearly distribution of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+----------+-----+----+\n",
      "|Critic_Score|Critics|User_Score|Users|Year|\n",
      "+------------+-------+----------+-----+----+\n",
      "|        71.0|    326|       8.0|  257|2001|\n",
      "|        69.0|    627|       8.0|  464|2002|\n",
      "|        70.0|    585|       8.0|  516|2003|\n",
      "|        69.0|    561|       8.0|  489|2004|\n",
      "|        69.0|    655|       8.0|  583|2005|\n",
      "|        67.0|    620|       7.0|  552|2006|\n",
      "|        66.0|    692|       7.0|  629|2007|\n",
      "|        66.0|    715|       7.0|  661|2008|\n",
      "|        68.0|    651|       7.0|  614|2009|\n",
      "|        67.0|    500|       7.0|  467|2010|\n",
      "|        69.0|    500|       7.0|  501|2011|\n",
      "|        73.0|    321|       7.0|  339|2012|\n",
      "|        71.0|    273|       6.0|  304|2013|\n",
      "|        71.0|    261|       7.0|  329|2014|\n",
      "|        73.0|    225|       7.0|  297|2015|\n",
      "|        73.0|    232|       7.0|  262|2016|\n",
      "+------------+-------+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = scores_df.filter(col(\"Year\") > 2000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values when Year_of_Release is less than 2000\n",
    "df = df.withColumn(\"Critic_Score\", when(df[\"Year_of_Release\"] < 2000, 0).otherwise(df[\"Critic_Score\"]))\n",
    "df = df.withColumn(\"Critic_Count\", when(df[\"Year_of_Release\"] < 2000, 0).otherwise(df[\"Critic_Count\"]))\n",
    "df = df.withColumn(\"User_Score\", when(df[\"Year_of_Release\"] < 2000, 0).otherwise(df[\"User_Score\"]))\n",
    "df = df.withColumn(\"User_Count\", when(df[\"Year_of_Release\"] < 2000, 0).otherwise(df[\"User_Count\"]))\n",
    "\n",
    "# Replace missing values\n",
    "critic_score_mean = scores_df.filter(col(\"Year\") > 2000).agg({\"Critic_Score\": \"mean\"}).collect()[0][0]\n",
    "user_score_mean = scores_df.filter(col(\"Year\") > 2000).agg({\"User_Score\": \"mean\"}).collect()[0][0]\n",
    "df = df.withColumn(\"Critic_Score\", when(df[\"Critic_Score\"].isNull(), critic_score_mean).otherwise(df[\"Critic_Score\"]))\n",
    "df = df.withColumn(\"Critic_Count\", when(df[\"Critic_Count\"].isNull(), 0).otherwise(df[\"Critic_Count\"]))\n",
    "df = df.withColumn(\"User_Score\", when(df[\"User_Score\"].isNull(), user_score_mean).otherwise(df[\"User_Score\"]))\n",
    "df = df.withColumn(\"User_Count\", when(df[\"User_Count\"].isNull(), 0).otherwise(df[\"User_Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+------+\n",
      "|Name|Platform|Year_of_Release|Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|Critic_Score|Critic_Count|User_Score|User_Count|Rating|\n",
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+------+\n",
      "|   0|       0|              0|    0|        0|       0|       0|       0|          0|           0|           0|           0|         0|         0|     0|\n",
      "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_counts = df.select([sum(when(isnull(c) | isnan(c) | (col(c) == \"\"), 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "na_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "columns = ['Year_of_Release', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales', 'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count']\n",
    "\n",
    "# Conver the df columns to int/float\n",
    "df = convertColumn(df, columns, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Year_of_Release: float (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- NA_Sales: float (nullable = true)\n",
      " |-- EU_Sales: float (nullable = true)\n",
      " |-- JP_Sales: float (nullable = true)\n",
      " |-- Other_Sales: float (nullable = true)\n",
      " |-- Global_Sales: float (nullable = true)\n",
      " |-- Critic_Score: float (nullable = true)\n",
      " |-- Critic_Count: float (nullable = true)\n",
      " |-- User_Score: float (nullable = true)\n",
      " |-- User_Count: float (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(temp_path, target_path, df):\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_path)\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "\n",
    "    # get the part file generated by spark write\n",
    "    fs = Path(temp_path).getFileSystem(sc._jsc.hadoopConfiguration())\n",
    "    csv_part_file = fs.globStatus(Path(temp_path + \"/part*\"))[0].getPath()\n",
    "\n",
    "    # move and rename the file\n",
    "    fs.rename(csv_part_file, Path(target_path))\n",
    "    fs.delete(Path(temp_path), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"data/__temp\"\n",
    "target_path = \"data/Video_Games_Sales_Fixed.csv\"\n",
    "\n",
    "save_csv(temp_path, target_path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df, year):\n",
    "    train_df = df.filter(df['Year_of_Release'] < year)\n",
    "    test_df = df.filter(df['Year_of_Release'] == year)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preProcess(fixed_df): \n",
    "    # Models Pre-Process\n",
    "    # ----------------\n",
    "\n",
    "    # String Indexer\n",
    "    indexer = StringIndexer(inputCols=['Platform', 'Genre', 'Publisher', 'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count', 'Rating'], \n",
    "                                    outputCols=['Platform_numeric', 'Genre_numeric', 'Publisher_numeric', 'Critic_Score_numeric',\n",
    "                                                'Critic_Count_numeric', 'User_Score_numeric', 'User_Count_numeric', 'Rating_numeric'])\n",
    "    indexer_fitted = indexer.fit(fixed_df)\n",
    "    df_indexed = indexer_fitted.transform(fixed_df)\n",
    "\n",
    "    # OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=['Platform_numeric', 'Genre_numeric', 'Publisher_numeric', 'Critic_Score_numeric',\n",
    "                                               'Critic_Count_numeric', 'User_Score_numeric', 'User_Count_numeric', 'Rating_numeric'], \n",
    "                                   outputCols=['Platform_Vec', 'Genre_Vec', 'Publisher_Vec', 'Critic_Score_Vec',\n",
    "                                               'Critic_Count_Vec', 'User_Score_Vec', 'User_Count_Vec', 'Rating_Vec'])\n",
    "    model = encoder.fit(df_indexed)\n",
    "    encoded = model.transform(df_indexed)\n",
    "\n",
    "    # VectorAssembler\n",
    "    vectorAssembler = VectorAssembler(inputCols = ['Platform_Vec', 'Genre_Vec', 'Publisher_Vec', 'Critic_Score_Vec',\n",
    "                                               'Critic_Count_Vec', 'User_Score_Vec', 'User_Count_Vec', 'Rating_Vec'], \n",
    "                                  outputCol = 'features')\n",
    "    v_df = vectorAssembler.transform(encoded)\n",
    "    v_df = v_df.select(['Name', 'Year_of_Release', 'features', 'Global_Sales'])\n",
    "    \n",
    "    return v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, year):\n",
    "    models_results = []\n",
    " \n",
    "    # Train Test split  \n",
    "    train_df, test_df = get_train_test_data(df, year)\n",
    "\n",
    "    # Running Models\n",
    "    # ---------------\n",
    "\n",
    "    # Linear Regression\n",
    "    lr = LinearRegression(featuresCol='features', labelCol='Global_Sales')\n",
    "    lr_model = lr.fit(train_df)\n",
    "    lr_predictions = lr_model.transform(test_df)\n",
    "    lr_prediction_rows = lr_predictions.select(\"prediction\").collect()\n",
    "    lr_prediction_list = []\n",
    "    for row in lr_prediction_rows:\n",
    "        lr_prediction_list.append(row.asDict()['prediction'])\n",
    "    lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Global_Sales\", metricName=\"rmse\")\n",
    "    rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "        \n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'LinearRegression',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": lr_prediction_list\n",
    "    })\n",
    "    \n",
    "    # Ridge Regression\n",
    "    ridge = LinearRegression(featuresCol='features', labelCol='Global_Sales', regParam=0.2, elasticNetParam=0)\n",
    "    ridge_model = ridge.fit(train_df)\n",
    "    ridge_predictions = ridge_model.transform(test_df)\n",
    "    ridge_prediction_rows = ridge_predictions.select(\"prediction\").collect()\n",
    "    ridge_prediction_list = []\n",
    "    for row in ridge_prediction_rows:\n",
    "        ridge_prediction_list.append(row.asDict()['prediction'])\n",
    "    ridge_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Global_Sales\", metricName=\"rmse\")\n",
    "    rmse = ridge_evaluator.evaluate(ridge_predictions)\n",
    "        \n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'RidgeRegression',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": ridge_prediction_list\n",
    "    })\n",
    "    \n",
    "    # Lasso Regression\n",
    "    lasso = LinearRegression(featuresCol='features', labelCol='Global_Sales', regParam=0.2, elasticNetParam=1)\n",
    "    lasso_model = lasso.fit(train_df)\n",
    "    lasso_predictions = lasso_model.transform(test_df)\n",
    "    lasso_prediction_rows = lasso_predictions.select(\"prediction\").collect()\n",
    "    lasso_prediction_list = []\n",
    "    for row in lasso_prediction_rows:\n",
    "        lasso_prediction_list.append(row.asDict()['prediction'])\n",
    "    lasso_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Global_Sales\", metricName=\"rmse\")\n",
    "    rmse = lasso_evaluator.evaluate(lasso_predictions)\n",
    "        \n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'LassoRegression',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": lasso_prediction_list\n",
    "    })\n",
    "    \n",
    "    # Decision Tree Regression\n",
    "    dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'Global_Sales')\n",
    "    dt_model = dt.fit(train_df)\n",
    "    dt_predictions = dt_model.transform(test_df)\n",
    "    dt_prediction_rows = dt_predictions.select(\"prediction\").collect()\n",
    "    dt_prediction_list = []\n",
    "    for row in dt_prediction_rows:\n",
    "        dt_prediction_list.append(row.asDict()['prediction'])\n",
    "    dt_evaluator = RegressionEvaluator(labelCol=\"Global_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'DecisionTreeRegressor',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": dt_prediction_list\n",
    "    })\n",
    "\n",
    "    # Gradient-boosted tree regression\n",
    "    gbt = GBTRegressor(featuresCol = 'features', labelCol = 'Global_Sales', maxIter=10)\n",
    "    gbt_model = gbt.fit(train_df)\n",
    "    gbt_predictions = gbt_model.transform(test_df)\n",
    "    gbt_prediction_rows = gbt_predictions.select(\"prediction\").collect()\n",
    "    gbt_prediction_list = []\n",
    "    for row in gbt_prediction_rows:\n",
    "        gbt_prediction_list.append(row.asDict()['prediction'])\n",
    "    gbt_evaluator = RegressionEvaluator(labelCol=\"Global_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'GBTRegressor',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": gbt_prediction_list\n",
    "    })\n",
    "    \n",
    "    # Random Forest regression\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Global_Sales\", numTrees=10, maxDepth=5)\n",
    "    rf_model = rf.fit(train_df)\n",
    "    rf_predictions = rf_model.transform(test_df)\n",
    "    rf_prediction_rows = rf_predictions.select(\"prediction\").collect()\n",
    "    rf_prediction_list = []\n",
    "    for row in rf_prediction_rows:\n",
    "        rf_prediction_list.append(row.asDict()['prediction'])\n",
    "    rf_evaluator = RegressionEvaluator(labelCol=\"Global_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "    # update models_results data\n",
    "    models_results.append({\n",
    "        \"model\": 'RandomForestRegressor',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": rf_prediction_list\n",
    "    })\n",
    "\n",
    "    # XGBoost regression\n",
    "    xgb_regressor = SparkXGBRegressor(features_col=\"features\", label_col=\"Global_Sales\")\n",
    "    xgb_model = xgb_regressor.fit(train_df)\n",
    "    xgb_predictions = xgb_model.transform(test_df)\n",
    "    xgb_predictions_rows = xgb_predictions.select(\"prediction\").collect()\n",
    "    xgb_prediction_list = []\n",
    "    for row in xgb_predictions_rows:\n",
    "        xgb_prediction_list.append(row.asDict()['prediction'])\n",
    "    xgb_evaluator = RegressionEvaluator(labelCol=\"Global_Sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = xgb_evaluator.evaluate(xgb_predictions)\n",
    "    \n",
    "    # update model data\n",
    "    models_results.append({\n",
    "        \"model\": 'SparkXGBRegressor',\n",
    "        \"year\": year,\n",
    "        \"rmse\": float(rmse),\n",
    "        \"predictions\": xgb_prediction_list\n",
    "    })\n",
    "    \n",
    "    # create a DataFrame from the RDD\n",
    "    model_df = spark.createDataFrame(models_results)    \n",
    "    model_df = model_df.withColumn(\"predictions_str\", concat_ws(\",\", col(\"predictions\")))\n",
    "    model_df = model_df.drop(\"predictions\")\n",
    "    \n",
    "    # write the DataFrame to a CSV file\n",
    "    temp_path = \"results/__temp\"\n",
    "    file_name = f'results/{year}.csv'\n",
    "\n",
    "    save_csv(temp_path, file_name, model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic salesHistory.\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/kafka/kafka_2.13-3.2.1/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic salesHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic predictions.\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/kafka/kafka_2.13-3.2.1/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sales history data\n",
    "with open('data/Video_Games_Sales_Fixed.csv', 'r') as f:\n",
    "    # Read the lines of the file\n",
    "    lines = f.readlines()\n",
    "\n",
    "    sales_history_data = {}\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over the lines\n",
    "    for line in lines:\n",
    "        # save each line in a dict\n",
    "        sales_history_data[counter] = line.replace(\"\\n\",\"\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sales_history_data from dict to pyspark df\n",
    "def dict_to_df(data):    \n",
    "    # split the values of the dictionary into a list of strings\n",
    "    rows = list(data.values())\n",
    "\n",
    "    # split the header string into a list of column names\n",
    "    header = rows[0].split(',')\n",
    "\n",
    "    # limit the number of columns to 15\n",
    "    header = header[:15]\n",
    "    rows = [row.split(',')[:15] for row in rows[1:]]\n",
    "\n",
    "    # create a PySpark DF by passing list of tuples, where each tuple contains the column values for a given row\n",
    "    df_Fixed = spark.createDataFrame(rows, header)\n",
    "    \n",
    "    return df_Fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data into a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Kafka producer configuration\n",
    "producer_conf = {\n",
    "        'bootstrap.servers': 'localhost:9092',\n",
    "        'client.id': 'python-producer'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(topic, data):\n",
    "    # Create the Kafka producer instance\n",
    "    producer = Producer(producer_conf)\n",
    "\n",
    "    # Serialize the dictionary to a JSON string\n",
    "    json_message = json.dumps(data)\n",
    "\n",
    "    # Split the JSON string into smaller chunks\n",
    "    chunk_size = 100\n",
    "    for i in range(0, len(json_message), chunk_size):\n",
    "        chunk = json_message[i:i + chunk_size]\n",
    "        producer.produce(topic, key='chunk-{}'.format(i), value=chunk)\n",
    "\n",
    "    producer.produce(topic, key='END', value='END')\n",
    "\n",
    "    # Wait for any outstanding messages to be delivered and delivery report callbacks to be received\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('salesHistory', sales_history_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_data(topic, consumer):\n",
    "    # Subscribe to the topic\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    # Initialize a variable to store the reassembled message\n",
    "    reassembled_message = ''\n",
    "\n",
    "    # Poll for messages\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "\n",
    "        if msg is None: continue\n",
    "            \n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError.PARTITION_EOF:\n",
    "                print('Reached end of partition event for {} [{}] at offset {}'.format(\n",
    "                    msg.topic(), msg.partition(), msg.offset()))\n",
    "            else:\n",
    "                print('Error while polling for messages: {}'.format(msg.error()))\n",
    "            \n",
    "        else:\n",
    "            if (msg.value().decode('utf-8')) == 'END':\n",
    "                break\n",
    "            reassembled_message += msg.value().decode('utf-8')\n",
    "\n",
    "    # Deserialize the reassembled message from JSON to a dictionary\n",
    "    deserialized_message = json.loads(reassembled_message)\n",
    "    # cast the dict keys into int\n",
    "    sales_history_data_consumed = {int(key): value for key, value in deserialized_message.items()}\n",
    "\n",
    "    # return the deserialized dictionary\n",
    "    return sales_history_data_consumed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consume sales history and produce prediction\n",
    "def main_func(consumer, producer, topics, year):\n",
    "    sales_history_data = consume_data(topics[0], consumer)\n",
    "    \n",
    "    # convert sales_history_data to pyspark df\n",
    "    sales_history_df = dict_to_df(sales_history_data)\n",
    "    \n",
    "    # get the best prediction model\n",
    "    v_df = df_preProcess(sales_history_df)\n",
    "    v_df = convertColumn(v_df, ['Global_Sales'], \"float\")\n",
    "    run_model(v_df, year)\n",
    "    df_model = spark.read.format(\"csv\").option(\"header\",\"true\").load(f'results/{year}.csv')\n",
    "    \n",
    "    # send to predictions topic\n",
    "    for row in df_model.toJSON().collect():\n",
    "        producer.produce(topic=topics[1], key=None, value=row)\n",
    "    \n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Mario Land,GB,1989.0,Platform,Nintendo,10.83,2.71,4.18,0.42,18.14,0.0,0.0,0.0,0.0,NR\n",
      "PASS 1\n",
      "PASS 2\n",
      "23/03/05 12:10:57 WARN Instrumentation: [e80558cd] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/03/05 12:10:57 WARN Instrumentation: [e80558cd] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:11:23] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 3\n",
      "PASS 4\n"
     ]
    }
   ],
   "source": [
    "random_id = random.randint(1, 10000)\n",
    "\n",
    "# Define the Kafka consumer configuration\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': f'my_group{random_id}',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Define the Kafka producer configuration\n",
    "producer_conf = {\n",
    "        'bootstrap.servers': 'localhost:9092',\n",
    "        'client.id': 'python_producer'\n",
    "}\n",
    "\n",
    "# create the Kafka producer instance\n",
    "p1 = Producer(producer_conf)\n",
    "\n",
    "# create the Kafka consumer instance\n",
    "c1 = Consumer(consumer_conf)\n",
    "\n",
    "# choose prediction year\n",
    "year = 2016\n",
    "\n",
    "# run the consume loop function\n",
    "main_func(c1, p1, ['salesHistory', 'predictions'], year)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a0e92e69bdb2ed0d947361424263fc828fa5c075b2d465730d0fd88045b73eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
